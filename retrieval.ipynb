{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Split document into lines\n",
    "    chunks = document.split('\\n')\n",
    "    # Remove any empty lines\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "# Load and preprocess the document\n",
    "document = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website-edited.txt\")\n",
    "chunks = preprocess_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Vectorize the chunks\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "# Store vectors and chunks\n",
    "index = {i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.D.5. explain their viewpoint clearly and concisely;', '7.E.3. After the spirit timeout:', '20.E. If a novice player commits an infraction out of sincere ignorance of the rules, it should be common practice to stop play and explain the infraction.', '2.H. In the case where a novice player commits an infraction out of ignorance of the rules, experienced players are obliged to explain the infraction and clarify what should happen.', '15.A.5.b. Specific Rules:']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, vectorizer, X, index, top_n=5):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    relevant_indices = np.argsort(similarities, axis=0)[-top_n:][::-1]\n",
    "    return [index[i] for i in relevant_indices]\n",
    "\n",
    "query = \"Explain the timeout rules\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, vectorizer, X, index)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Split document into lines\n",
    "    chunks = document.split('\\n')\n",
    "    # Remove any empty lines\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "# Load and preprocess the document\n",
    "document = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website-edited.txt\")\n",
    "chunks = preprocess_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 15:45:57.597059: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 15:45:57.604226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741293957.612157   33491 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741293957.614471   33491 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 15:45:57.624512: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15.A.1. The stall count consists of announcing “stalling” and counting from one to ten loudly enough for the thrower to hear.', '15.A.5. If a stall count is interrupted by a call, the thrower and marker are responsible for agreeing on the correct count before the check. The count reached is the last number fully uttered by the marker before the call. The count is resumed with the word “stalling” followed by the number listed below:', '7.D.4.a.2. If the technical timeout stopped play, the count resumes at the stall count reached plus one, or at six if over five.', '15.A.2.b. However, unless 15.A.2.a applies, the stall count may not be initiated or resumed before a pivot is established:', '15.B.6.c. If this (15.B.6.b) occurs in the same possession following a contested stall (either due to 15.B.6.a or 15.A.3.b), the stall count resumes at six.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the model\n",
    "model_retriever = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Vectorize the chunks\n",
    "chunk_embeddings = model_retriever.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "# Store vectors and chunks\n",
    "index = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "def retrieve_relevant_chunks(query, model_retriever, chunk_embeddings, index, top_n=5):\n",
    "    query_embedding = model_retriever.encode(query, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "    similarities = similarities.cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "    relevant_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    return [index[i] for i in relevant_indices]\n",
    "\n",
    "# query = \"Explain the timeout rules\"\n",
    "query = \"What is the stall count?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model_retriever, chunk_embeddings, index)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model_generator = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Set pad_token_id to eos_token_id\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def generate_response(query, relevant_chunks, model_generator, tokenizer):\n",
    "    # Combine the relevant chunks into a single context\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = inputs.ne(tokenizer.pad_token_id).long()  # Create attention mask\n",
    "    outputs = model_generator.generate(inputs, attention_mask=attention_mask, max_length=500, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "query = \"Explain the timeout rules\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model_retriever, chunk_embeddings, index)\n",
    "response = generate_response(query, relevant_chunks, model_generator, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env-2025-03-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
