{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Split document into lines\n",
    "    chunks = document.split('\\n')\n",
    "    # Remove any empty lines\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "# Load and preprocess the document\n",
    "document = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website-edited.txt\")\n",
    "chunks = preprocess_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Vectorize the chunks\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(chunks)\n",
    "\n",
    "# Store vectors and chunks\n",
    "index = {i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.D.5. explain their viewpoint clearly and concisely;', '7.E.3. After the spirit timeout:', '20.E. If a novice player commits an infraction out of sincere ignorance of the rules, it should be common practice to stop play and explain the infraction.', '2.H. In the case where a novice player commits an infraction out of ignorance of the rules, experienced players are obliged to explain the infraction and clarify what should happen.', '15.A.5.b. Specific Rules:']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_relevant_chunks(query, vectorizer, X, index, top_n=5):\n",
    "    query_vec = vectorizer.transform([query])\n",
    "    similarities = cosine_similarity(query_vec, X).flatten()\n",
    "    relevant_indices = np.argsort(similarities, axis=0)[-top_n:][::-1]\n",
    "    return [index[i] for i in relevant_indices]\n",
    "\n",
    "query = \"Explain the timeout rules\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, vectorizer, X, index)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "def preprocess_document(document):\n",
    "    # Split document into lines\n",
    "    chunks = document.split('\\n')\n",
    "    # Remove any empty lines\n",
    "    chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "    return chunks\n",
    "\n",
    "# Load and preprocess the document\n",
    "document = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website-edited.txt\")\n",
    "chunks = preprocess_document(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15.A.1. The stall count consists of announcing “stalling” and counting from one to ten loudly enough for the thrower to hear.', '15.A.5. If a stall count is interrupted by a call, the thrower and marker are responsible for agreeing on the correct count before the check. The count reached is the last number fully uttered by the marker before the call. The count is resumed with the word “stalling” followed by the number listed below:', '7.D.4.a.2. If the technical timeout stopped play, the count resumes at the stall count reached plus one, or at six if over five.', '15.A.2.b. However, unless 15.A.2.a applies, the stall count may not be initiated or resumed before a pivot is established:', '15.B.6.c. If this (15.B.6.b) occurs in the same possession following a contested stall (either due to 15.B.6.a or 15.A.3.b), the stall count resumes at six.']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Vectorize the chunks\n",
    "chunk_embeddings = model.encode(chunks, convert_to_tensor=True)\n",
    "\n",
    "# Store vectors and chunks\n",
    "index = {i: chunk for i, chunk in enumerate(chunks)}\n",
    "\n",
    "def retrieve_relevant_chunks(query, model, chunk_embeddings, index, top_n=5):\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, chunk_embeddings)[0]\n",
    "    similarities = similarities.cpu().numpy()  # Move to CPU and convert to NumPy array\n",
    "    relevant_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "    return [index[i] for i in relevant_indices]\n",
    "\n",
    "# query = \"Explain the timeout rules\"\n",
    "query = \"What is the stall count?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model, chunk_embeddings, index)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "def generate_response(query, relevant_chunks, model, tokenizer):\n",
    "    # Combine the relevant chunks into a single context\n",
    "    context = \" \".join(relevant_chunks)\n",
    "    input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
    "    inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=500, num_return_sequences=1)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example query\n",
    "query = \"Explain the timeout rules\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query, model, chunk_embeddings, index)\n",
    "response = generate_response(query, relevant_chunks, model, tokenizer)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env-2025-03-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
