{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Load the rules text from the provided file\n",
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "\n",
    "# Fine-tune the model using each line of the rules.\n",
    "def fine_tune_model(model, tokenizer, rules_text):\n",
    "\n",
    "    # Determine the device to use.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Ensure the model is on the selected device.\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure the model is in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Tokenize each line in the rules.\n",
    "    tokens_per_line = [tokenizer(line, return_tensors=\"pt\").to(device) for line in rules_text.split('\\n')]\n",
    "\n",
    "    # Get the number of tokens for each line of the rules.\n",
    "    num_tokens_per_line = [len(tokens['input_ids'][0]) for tokens in tokens_per_line]\n",
    "\n",
    "    # Ensure the model can handle every line.\n",
    "    model_max_length = tokenizer.model_max_length\n",
    "    assert max(num_tokens_per_line) <= model_max_length, 'The maximum number of tokens in all lines is greater than the maximum number of tokens the model can handle.'\n",
    "\n",
    "    # Get the total number of lines i.e. number of token sets.\n",
    "    tot_num_tokens = len(num_tokens_per_line)\n",
    "\n",
    "    # Initialize the optimizer.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # For each line in the rules...\n",
    "    for itokens, tokens in enumerate(tokens_per_line):\n",
    "        num_tokens = len(tokens[\"input_ids\"][0])\n",
    "        print(f'On line {itokens + 1} of {tot_num_tokens}: number of tokens: {num_tokens}')\n",
    "        if num_tokens != 0:\n",
    "            outputs = model(**tokens, labels=tokens[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            print('Skipping line because it is blank.')\n",
    "\n",
    "\n",
    "def retrieve_relevant_section(query, vectorizer, rules_vectors, rules_sections):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = np.dot(query_vector, rules_vectors.T).toarray()[0]\n",
    "    # print(np.sort(similarities)[::-1])\n",
    "    print(similarities.shape)\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    # Print the top ten largest indices.\n",
    "    print(np.argsort(similarities)[::-1][:10])\n",
    "\n",
    "    return('Andrew\\'s favorite color is violet.')\n",
    "    \n",
    "    return ('The stall count is 74.')\n",
    "\n",
    "    return rules_sections[most_similar_index]\n",
    "\n",
    "\n",
    "def ask_question(question, vectorizer, rules_vectors, rules_sections, qa_pipeline):\n",
    "    relevant_section = retrieve_relevant_section(question, vectorizer, rules_vectors, rules_sections)\n",
    "    # input_text = f\"Context: {relevant_section}\\nQuestion: {question}\\nAnswer:\"\n",
    "    input_text = f\"{relevant_section} {question}\\n\"\n",
    "    print('AAAA')\n",
    "    response = qa_pipeline(input_text, max_length=100, num_return_sequences=1)\n",
    "    print('BBBB')\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "[100 101  92  95  49 438  65 209 233  94]\n",
      "AAAA\n",
      "BBBB\n",
      "Andrew's favorite color is violet. What is Andrew's favorite color?\n",
      "\n",
      "My favorite color is pink. The colors she calls a \"peachy hue\" are bright pink and bright pink.\n",
      "\n",
      "What is Andrew's favorite color?\n",
      "\n",
      "White, but I don't care what it isâ€”we're just going to tell you how to get that, and then they'll make you look better.\n",
      "\n",
      "What is Andrew's favorite color?\n",
      "\n",
      "White. Pretty much.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rules_text = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website.txt\")\n",
    "\n",
    "# Step 2: Preprocess the Text (if needed)\n",
    "# Here you can add any text preprocessing steps if required\n",
    "\n",
    "# Step 3: Fine-Tune GPT-2\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  # this downloads and caches (across sessions) some files including the model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  # this also downloads and caches things\n",
    "# fine_tune_model(model, tokenizer, rules_text)\n",
    "\n",
    "# Step 4: Implement Retrieval Mechanism\n",
    "# Split the rules text into sections for retrieval\n",
    "rules_sections = rules_text.split('\\n')\n",
    "\n",
    "# Create a TF-IDF vectorizer and fit it on the rules sections\n",
    "vectorizer = TfidfVectorizer().fit(rules_sections)\n",
    "rules_vectors = vectorizer.transform(rules_sections)\n",
    "\n",
    "# Step 5: Integrate Retrieval with Generation\n",
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example question\n",
    "# question = \"What is the stall count in ultimate frisbee?\"\n",
    "# question = \"What is the stall count?\"\n",
    "question = 'What is Andrew\\'s favorite color?'\n",
    "answer = ask_question(question, vectorizer, rules_vectors, rules_sections, qa_pipeline)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from PDF using PyMuPDF. Good but has newlines in paragraphs.\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "pdf_path = \"Official-Rules-of-Ultimate-2024-2025.pdf\"\n",
    "rules_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "with open(\"ultimate_frisbee_rules-pdf_extraction.txt\", \"w\") as file:\n",
    "    file.write(rules_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rules have been successfully extracted and saved to 'cleaned_ultimate_frisbee_rules.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Extraction from HTML using BeautifulSoup. Not so great.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content from the file\n",
    "with open('ultimate_frisbee_rules.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the main content of the rules\n",
    "rules_content = soup.find('div', {'id': 'rules-of-ultimate'}).get_text(separator='\\n')\n",
    "\n",
    "# Clean the extracted text\n",
    "def clean_text(text):\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Replace newlines in the middle of paragraphs with a space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Ensure paragraphs are separated by a single newline\n",
    "    text = text.replace('. ', '.\\n')\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_rules_content = clean_text(rules_content)\n",
    "\n",
    "# Save the cleaned rules to a text file\n",
    "with open('ultimate_frisbee_rules-html_extraction.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_rules_content)\n",
    "\n",
    "print(\"The rules have been successfully extracted and saved to 'cleaned_ultimate_frisbee_rules.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.6597366333007812 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.5771596431732178 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.5154917240142822 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.531101703643799 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.6688392162323 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.4295167922973633 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.516512632369995 seconds\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMatrix multiplication completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mgpu_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgpu_test\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create random tensors\u001b[39;00m\n\u001b[32m     12\u001b[39m a = torch.randn(\u001b[32m10000\u001b[39m, \u001b[32m10000\u001b[39m, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m b = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Perform matrix multiplication\u001b[39;00m\n\u001b[32m     16\u001b[39m start_time = time.time()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GPU test (pytorch)\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Define a simple matrix multiplication task\n",
    "def gpu_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create random tensors\n",
    "    a = torch.randn(10000, 10000, device=device)\n",
    "    b = torch.randn(10000, 10000, device=device)\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    start_time = time.time()\n",
    "    c = torch.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication completed in {end_time - start_time} seconds\")\n",
    "\n",
    "for i in range(10000):\n",
    "    gpu_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:00:32.762867: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 15:00:32.772080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741204832.781773   12855 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741204832.784905   12855 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 15:00:32.795874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "TensorFlow GPU details:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# GPU test (tensorflow)\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"TensorFlow GPU details:\")\n",
    "    for gpu in physical_devices:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs detected by TensorFlow.\")\n",
    "\n",
    "# Define a simple matrix multiplication task\n",
    "def gpu_test():\n",
    "    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create random tensors\n",
    "    with tf.device(device):\n",
    "        a = tf.random.normal([10000, 10000])\n",
    "        b = tf.random.normal([10000, 10000])\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        start_time = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication completed in {end_time - start_time} seconds\")\n",
    "\n",
    "for i in range(10000):\n",
    "    gpu_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env-2025-03-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
