{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Step 1: Load the rules text from the provided file\n",
    "def load_rules(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        rules_text = file.read()\n",
    "    return rules_text\n",
    "\n",
    "\n",
    "# Fine-tune the model using each line of the rules.\n",
    "def fine_tune_model(model, tokenizer, rules_text):\n",
    "\n",
    "    # Determine the device to use.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Ensure the model is on the selected device.\n",
    "    model.to(device)\n",
    "\n",
    "    # Ensure the model is in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Tokenize each line in the rules.\n",
    "    tokens_per_line = [tokenizer(line, return_tensors=\"pt\").to(device) for line in rules_text.split('\\n')]\n",
    "\n",
    "    # Get the number of tokens for each line of the rules.\n",
    "    num_tokens_per_line = [len(tokens['input_ids'][0]) for tokens in tokens_per_line]\n",
    "\n",
    "    # Ensure the model can handle every line.\n",
    "    model_max_length = tokenizer.model_max_length\n",
    "    assert max(num_tokens_per_line) <= model_max_length, 'The maximum number of tokens in all lines is greater than the maximum number of tokens the model can handle.'\n",
    "\n",
    "    # Get the total number of lines i.e. number of token sets.\n",
    "    tot_num_tokens = len(num_tokens_per_line)\n",
    "\n",
    "    # Initialize the optimizer.\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    # For each line in the rules...\n",
    "    for itokens, tokens in enumerate(tokens_per_line):\n",
    "        num_tokens = len(tokens[\"input_ids\"][0])\n",
    "        print(f'On line {itokens + 1} of {tot_num_tokens}: number of tokens: {num_tokens}')\n",
    "        if num_tokens != 0:\n",
    "            outputs = model(**tokens, labels=tokens[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        else:\n",
    "            print('Skipping line because it is blank.')\n",
    "\n",
    "\n",
    "def retrieve_relevant_section(query, vectorizer, rules_vectors, rules_sections):\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    similarities = np.dot(query_vector, rules_vectors.T).toarray()[0]\n",
    "    # print(np.sort(similarities)[::-1])\n",
    "    print(similarities.shape)\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    # Print the top ten largest indices.\n",
    "    print(np.argsort(similarities)[::-1][:10])\n",
    "\n",
    "    return('Andrew\\'s favorite color is violet.')\n",
    "    \n",
    "    return ('The stall count is 74.')\n",
    "\n",
    "    return rules_sections[most_similar_index]\n",
    "\n",
    "\n",
    "def ask_question(question, vectorizer, rules_vectors, rules_sections, qa_pipeline):\n",
    "    relevant_section = retrieve_relevant_section(question, vectorizer, rules_vectors, rules_sections)\n",
    "    # input_text = f\"Context: {relevant_section}\\nQuestion: {question}\\nAnswer:\"\n",
    "    input_text = f\"{relevant_section} {question}\\n\"\n",
    "    print('AAAA')\n",
    "    response = qa_pipeline(input_text, max_length=100, num_return_sequences=1)\n",
    "    print('BBBB')\n",
    "    return response[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'GPT2LMHeadModel' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DiffLlamaForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LlamaForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MistralForQuestionAnswering', 'MixtralForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NemotronForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'Qwen2ForQuestionAnswering', 'Qwen2MoeForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n",
      "/home/andrew/miniconda3/envs/conda_env-2025-03-03/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(672,)\n",
      "[100 101  92  95  49 438  65 209 233  94]\n",
      "AAAA\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A argument needs to be of type (SquadExample, dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Example question\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# question = \"What is the stall count in ultimate frisbee?\"\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# question = \"What is the stall count?\"\u001b[39;00m\n\u001b[32m     27\u001b[39m question = \u001b[33m'\u001b[39m\u001b[33mWhat is Andrew\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33ms favorite color?\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m answer = \u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrules_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrules_sections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqa_pipeline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mask_question\u001b[39m\u001b[34m(question, vectorizer, rules_vectors, rules_sections, qa_pipeline)\u001b[39m\n\u001b[32m     77\u001b[39m input_text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelevant_section\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAAAA\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m response = \u001b[43mqa_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mBBBB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/conda_env-2025-03-03/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:396\u001b[39m, in \u001b[36mQuestionAnsweringPipeline.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args:\n\u001b[32m    391\u001b[39m     warnings.warn(\n\u001b[32m    392\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    393\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    394\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m examples = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_args_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(examples, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(examples) == \u001b[32m1\u001b[39m:\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(examples[\u001b[32m0\u001b[39m], **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/conda_env-2025-03-03/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:227\u001b[39m, in \u001b[36mQuestionAnsweringArgumentHandler.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     inputs[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/conda_env-2025-03-03/lib/python3.12/site-packages/transformers/pipelines/question_answering.py:172\u001b[39m, in \u001b[36mQuestionAnsweringArgumentHandler.normalize\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    169\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` cannot be empty\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m QuestionAnsweringPipeline.create_sample(**item)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m argument needs to be of type (SquadExample, dict)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: A argument needs to be of type (SquadExample, dict)"
     ]
    }
   ],
   "source": [
    "rules_text = load_rules(\"ultimate_frisbee_rules-manual_copy_from_website.txt\")\n",
    "\n",
    "# Step 2: Preprocess the Text (if needed)\n",
    "# Here you can add any text preprocessing steps if required\n",
    "\n",
    "# Step 3: Fine-Tune GPT-2\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)  # this downloads and caches (across sessions) some files including the model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)  # this also downloads and caches things\n",
    "# fine_tune_model(model, tokenizer, rules_text)\n",
    "\n",
    "# Step 4: Implement Retrieval Mechanism\n",
    "# Split the rules text into sections for retrieval\n",
    "rules_sections = rules_text.split('\\n')\n",
    "\n",
    "# Create a TF-IDF vectorizer and fit it on the rules sections\n",
    "vectorizer = TfidfVectorizer().fit(rules_sections)\n",
    "rules_vectors = vectorizer.transform(rules_sections)\n",
    "\n",
    "# Step 5: Integrate Retrieval with Generation\n",
    "# Create a pipeline for question answering\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example question\n",
    "# question = \"What is the stall count in ultimate frisbee?\"\n",
    "# question = \"What is the stall count?\"\n",
    "question = 'What is Andrew\\'s favorite color?'\n",
    "answer = ask_question(question, vectorizer, rules_vectors, rules_sections, qa_pipeline)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction from PDF using PyMuPDF. Good but has newlines in paragraphs.\n",
    "\n",
    "# Extract Text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# Step 1: Extract text from the PDF\n",
    "pdf_path = \"Official-Rules-of-Ultimate-2024-2025.pdf\"\n",
    "rules_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Save the extracted text to a file\n",
    "with open(\"ultimate_frisbee_rules-pdf_extraction.txt\", \"w\") as file:\n",
    "    file.write(rules_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The rules have been successfully extracted and saved to 'cleaned_ultimate_frisbee_rules.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Extraction from HTML using BeautifulSoup. Not so great.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content from the file\n",
    "with open('ultimate_frisbee_rules.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the main content of the rules\n",
    "rules_content = soup.find('div', {'id': 'rules-of-ultimate'}).get_text(separator='\\n')\n",
    "\n",
    "# Clean the extracted text\n",
    "def clean_text(text):\n",
    "    # Replace multiple newlines with a single newline\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "    \n",
    "    # Replace newlines in the middle of paragraphs with a space\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # Ensure paragraphs are separated by a single newline\n",
    "    text = text.replace('. ', '.\\n')\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_rules_content = clean_text(rules_content)\n",
    "\n",
    "# Save the cleaned rules to a text file\n",
    "with open('ultimate_frisbee_rules-html_extraction.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(cleaned_rules_content)\n",
    "\n",
    "print(\"The rules have been successfully extracted and saved to 'cleaned_ultimate_frisbee_rules.txt'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.6597366333007812 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.5771596431732178 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.5154917240142822 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.531101703643799 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.6688392162323 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.4295167922973633 seconds\n",
      "Using device: cpu\n",
      "Matrix multiplication completed in 2.516512632369995 seconds\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMatrix multiplication completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m10000\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[43mgpu_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgpu_test\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Create random tensors\u001b[39;00m\n\u001b[32m     12\u001b[39m a = torch.randn(\u001b[32m10000\u001b[39m, \u001b[32m10000\u001b[39m, device=device)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m b = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Perform matrix multiplication\u001b[39;00m\n\u001b[32m     16\u001b[39m start_time = time.time()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# GPU test (pytorch)\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Define a simple matrix multiplication task\n",
    "def gpu_test():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create random tensors\n",
    "    a = torch.randn(10000, 10000, device=device)\n",
    "    b = torch.randn(10000, 10000, device=device)\n",
    "\n",
    "    # Perform matrix multiplication\n",
    "    start_time = time.time()\n",
    "    c = torch.matmul(a, b)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication completed in {end_time - start_time} seconds\")\n",
    "\n",
    "for i in range(10000):\n",
    "    gpu_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 15:00:32.762867: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 15:00:32.772080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741204832.781773   12855 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741204832.784905   12855 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 15:00:32.795874: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "TensorFlow GPU details:\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "# GPU test (tensorflow)\n",
    "\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Check if TensorFlow can access the GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))\n",
    "\n",
    "if physical_devices:\n",
    "    print(\"TensorFlow GPU details:\")\n",
    "    for gpu in physical_devices:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPUs detected by TensorFlow.\")\n",
    "\n",
    "# Define a simple matrix multiplication task\n",
    "def gpu_test():\n",
    "    device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create random tensors\n",
    "    with tf.device(device):\n",
    "        a = tf.random.normal([10000, 10000])\n",
    "        b = tf.random.normal([10000, 10000])\n",
    "\n",
    "        # Perform matrix multiplication\n",
    "        start_time = time.time()\n",
    "        c = tf.matmul(a, b)\n",
    "        end_time = time.time()\n",
    "\n",
    "    print(f\"Matrix multiplication completed in {end_time - start_time} seconds\")\n",
    "\n",
    "for i in range(10000):\n",
    "    gpu_test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env-2025-03-03",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
